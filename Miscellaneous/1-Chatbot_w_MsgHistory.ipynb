{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  ### loading all enviornment variables\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001A30959B310>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001A3096DBCD0>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Access the Chat Model from Groq API\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Ankur,\\n\\nIt's great to meet you!  \\n\\nI understand you deliver trainings on Gen AI. That's a really exciting field! \\n\\nWhat kind of trainings do you offer? Do you focus on a specific aspect of Gen AI, like text generation, image creation, or something else?  \\n\\nI'm always interested in learning more about how people are using AI, so I'd love to hear more about your work.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 22, 'total_tokens': 118, 'completion_time': 0.174545455, 'prompt_time': 0.002143968, 'queue_time': 0.256592421, 'total_time': 0.176689423}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--4a7344c6-ddb1-4516-80f5-27d1fc83fe01-0', usage_metadata={'input_tokens': 22, 'output_tokens': 96, 'total_tokens': 118})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi my name is Ankur. I deliver trainings on Gen AI\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Ankur, and you deliver trainings on Generative AI (Gen AI)! ðŸ˜Š  \\n\\nIs there anything else you'd like to tell me about your work?  I'm curious to learn more! \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 135, 'total_tokens': 184, 'completion_time': 0.089090909, 'prompt_time': 0.008599421, 'queue_time': 0.261364041, 'total_time': 0.09769033}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--4a0b229f-62b2-4efd-80e6-1e18a65b4d98-0', usage_metadata={'input_tokens': 135, 'output_tokens': 49, 'total_tokens': 184})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"Hi my name is Ankur. I deliver trainings on Gen AI\"),\n",
    "              AIMessage(content=\"Hi Ankur,\\n\\nIt's great to meet you!  \\n\\nI understand you deliver trainings on Gen AI. That's a really exciting field! \\n\\nWhat kind of trainings do you offer? Do you focus on a specific aspect of Gen AI, like text generation, image creation, or something else?  \\n\\nI'm always interested in learning more about how people are using AI, so I'd love to hear more about your work.\\n\"),\n",
    "              HumanMessage(content=\"Whats my name and what do I do?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}   ### it keeps a track of sessions between user and model\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:     ### New session to be created if this condition is true\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logic to create session ID\n",
    "config = {\"configurable\" : {\"session_id\" : \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Hi, My name is Ankur and I deliver Gen AI trainings.\")] , config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Ankur,\\n\\nIt's nice to meet you! \\n\\nThat's fascinating work you do. Delivering Gen AI trainings must be incredibly rewarding. What kind of trainings do you focus on? Are they for technical professionals, business leaders, or a broader audience? \\n\\nI'm always eager to learn more about how people are using and understanding AI.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, My name is Ankur and I deliver Gen AI trainings.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Ankur,\\n\\nIt's nice to meet you! \\n\\nThat's fascinating work you do. Delivering Gen AI trainings must be incredibly rewarding. What kind of trainings do you focus on? Are they for technical professionals, business leaders, or a broader audience? \\n\\nI'm always eager to learn more about how people are using and understanding AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 23, 'total_tokens': 101, 'completion_time': 0.141818182, 'prompt_time': 0.002150428, 'queue_time': 0.26064566100000003, 'total_time': 0.14396861}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--44d3a052-553d-45ff-896d-36debe1a6068-0', usage_metadata={'input_tokens': 23, 'output_tokens': 78, 'total_tokens': 101})])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Ankur, and you deliver Gen AI trainings. ðŸ˜Š  I seem to be remembering that information correctly! \\n\\nIs there anything else you'd like to tell me about your work, or perhaps a specific Gen AI topic you're interested in discussing?  I'm here to chat!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 117, 'total_tokens': 182, 'completion_time': 0.118181818, 'prompt_time': 0.005364004, 'queue_time': 0.26285172700000003, 'total_time': 0.123545822}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac369026-6bb2-4280-b313-db3a687cec00-0', usage_metadata={'input_tokens': 117, 'output_tokens': 65, 'total_tokens': 182})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([HumanMessage(content=\"Whats my name and what I do?\")], config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name or what you do. If you'd like to tell me, I'm happy to learn!\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Change the config ---> session id\n",
    "\n",
    "config1 = {\"configurable\" : {\"session_id\" : \"chat2\"}}\n",
    "response = with_message_history.invoke([HumanMessage(content=\"Whats my name and what I do?\")] , config = config1)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Jhon, it's nice to meet you!  What can I do for you today? ðŸ˜Š \\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"My name is Jhon\")],\n",
    "                            config=config1)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You told me your name is Jhon.  \\n\\nSince I don't have access to any information about you outside of our current conversation, I don't know what you do. \\n\\nWould you like to tell me? ðŸ˜Š  \\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Whats my name and what I do?\")] , config = config1)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, My name is Ankur and I deliver Gen AI trainings.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Ankur,\\n\\nIt's nice to meet you! \\n\\nThat's fascinating work you do. Delivering Gen AI trainings must be incredibly rewarding. What kind of trainings do you focus on? Are they for technical professionals, business leaders, or a broader audience? \\n\\nI'm always eager to learn more about how people are using and understanding AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 23, 'total_tokens': 101, 'completion_time': 0.141818182, 'prompt_time': 0.002150428, 'queue_time': 0.26064566100000003, 'total_time': 0.14396861}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--44d3a052-553d-45ff-896d-36debe1a6068-0', usage_metadata={'input_tokens': 23, 'output_tokens': 78, 'total_tokens': 101}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You are Ankur, and you deliver Gen AI trainings. ðŸ˜Š  I seem to be remembering that information correctly! \\n\\nIs there anything else you'd like to tell me about your work, or perhaps a specific Gen AI topic you're interested in discussing?  I'm here to chat!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 117, 'total_tokens': 182, 'completion_time': 0.118181818, 'prompt_time': 0.005364004, 'queue_time': 0.26285172700000003, 'total_time': 0.123545822}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac369026-6bb2-4280-b313-db3a687cec00-0', usage_metadata={'input_tokens': 117, 'output_tokens': 65, 'total_tokens': 182})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name or what you do. If you'd like to tell me, I'm happy to learn!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 17, 'total_tokens': 58, 'completion_time': 0.074545455, 'prompt_time': 0.001929078, 'queue_time': 0.258541852, 'total_time': 0.076474533}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1e0e7a13-376b-4e2d-ab74-4183ddf5f804-0', usage_metadata={'input_tokens': 17, 'output_tokens': 41, 'total_tokens': 58}), HumanMessage(content='My name is Jhon', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Jhon, it's nice to meet you!  What can I do for you today? ðŸ˜Š \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 71, 'total_tokens': 98, 'completion_time': 0.049090909, 'prompt_time': 0.004126124, 'queue_time': 0.305708846, 'total_time': 0.053217033}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--c1164a18-80f9-4618-8dea-6814184122fc-0', usage_metadata={'input_tokens': 71, 'output_tokens': 27, 'total_tokens': 98}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You told me your name is Jhon.  \\n\\nSince I don't have access to any information about you outside of our current conversation, I don't know what you do. \\n\\nWould you like to tell me? ðŸ˜Š  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 113, 'total_tokens': 165, 'completion_time': 0.094545455, 'prompt_time': 0.005238704, 'queue_time': 0.26003697600000003, 'total_time': 0.099784159}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--0f6ef5a4-628c-43b3-bfe1-0b17199183ef-0', usage_metadata={'input_tokens': 113, 'output_tokens': 52, 'total_tokens': 165})])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative approach by using Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\" , \"You are a helpful assistant. Answer all the questions to the best of your ability\"),\n",
    "     MessagesPlaceholder(variable_name=\"messages\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Ankr! It's nice to meet you.  \\n\\nI'm ready to answer your questions to the best of my ability. Just ask away! ðŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 30, 'total_tokens': 69, 'completion_time': 0.070909091, 'prompt_time': 0.002104398, 'queue_time': 0.258621629, 'total_time': 0.073013489}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--93104ffc-d9f5-4979-b25d-c304864596da-0', usage_metadata={'input_tokens': 30, 'output_tokens': 39, 'total_tokens': 69})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\" : [HumanMessage(content = \"My name is Ankr\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and don't know your name.  \\n\\nWhat can I call you? ðŸ˜Š  \\n\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3 = {\"configurable\" : {\"session_id\" : \"chat3\"}}\n",
    "response = with_message_history.invoke([HumanMessage(content=\"Whats my name?\")] , config = config3)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, My name is Ankur and I deliver Gen AI trainings.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Ankur,\\n\\nIt's nice to meet you! \\n\\nThat's fascinating work you do. Delivering Gen AI trainings must be incredibly rewarding. What kind of trainings do you focus on? Are they for technical professionals, business leaders, or a broader audience? \\n\\nI'm always eager to learn more about how people are using and understanding AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 23, 'total_tokens': 101, 'completion_time': 0.141818182, 'prompt_time': 0.002150428, 'queue_time': 0.26064566100000003, 'total_time': 0.14396861}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--44d3a052-553d-45ff-896d-36debe1a6068-0', usage_metadata={'input_tokens': 23, 'output_tokens': 78, 'total_tokens': 101}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You are Ankur, and you deliver Gen AI trainings. ðŸ˜Š  I seem to be remembering that information correctly! \\n\\nIs there anything else you'd like to tell me about your work, or perhaps a specific Gen AI topic you're interested in discussing?  I'm here to chat!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 117, 'total_tokens': 182, 'completion_time': 0.118181818, 'prompt_time': 0.005364004, 'queue_time': 0.26285172700000003, 'total_time': 0.123545822}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac369026-6bb2-4280-b313-db3a687cec00-0', usage_metadata={'input_tokens': 117, 'output_tokens': 65, 'total_tokens': 182})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name or what you do. If you'd like to tell me, I'm happy to learn!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 17, 'total_tokens': 58, 'completion_time': 0.074545455, 'prompt_time': 0.001929078, 'queue_time': 0.258541852, 'total_time': 0.076474533}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1e0e7a13-376b-4e2d-ab74-4183ddf5f804-0', usage_metadata={'input_tokens': 17, 'output_tokens': 41, 'total_tokens': 58}), HumanMessage(content='My name is Jhon', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Jhon, it's nice to meet you!  What can I do for you today? ðŸ˜Š \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 71, 'total_tokens': 98, 'completion_time': 0.049090909, 'prompt_time': 0.004126124, 'queue_time': 0.305708846, 'total_time': 0.053217033}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--c1164a18-80f9-4618-8dea-6814184122fc-0', usage_metadata={'input_tokens': 71, 'output_tokens': 27, 'total_tokens': 98}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You told me your name is Jhon.  \\n\\nSince I don't have access to any information about you outside of our current conversation, I don't know what you do. \\n\\nWould you like to tell me? ðŸ˜Š  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 113, 'total_tokens': 165, 'completion_time': 0.094545455, 'prompt_time': 0.005238704, 'queue_time': 0.26003697600000003, 'total_time': 0.099784159}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--0f6ef5a4-628c-43b3-bfe1-0b17199183ef-0', usage_metadata={'input_tokens': 113, 'output_tokens': 52, 'total_tokens': 165})]),\n",
       " 'chat3': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and don't know your name.  \\n\\nWhat can I call you? ðŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 29, 'total_tokens': 62, 'completion_time': 0.06, 'prompt_time': 0.002082678, 'queue_time': 0.263287962, 'total_time': 0.062082678}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--86bff853-8b4e-4be1-8562-13b26baeff2d-0', usage_metadata={'input_tokens': 29, 'output_tokens': 33, 'total_tokens': 62})])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, Arun!\\n\\nIs there anything I can help you with today?  \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"My name is Arun\")] , config = config3)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your name is Arun!  ðŸ˜Š \\n\\nDo you need me to remember anything else?\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Whats my name?\")] , config = config3)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, My name is Ankur and I deliver Gen AI trainings.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Ankur,\\n\\nIt's nice to meet you! \\n\\nThat's fascinating work you do. Delivering Gen AI trainings must be incredibly rewarding. What kind of trainings do you focus on? Are they for technical professionals, business leaders, or a broader audience? \\n\\nI'm always eager to learn more about how people are using and understanding AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 23, 'total_tokens': 101, 'completion_time': 0.141818182, 'prompt_time': 0.002150428, 'queue_time': 0.26064566100000003, 'total_time': 0.14396861}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--44d3a052-553d-45ff-896d-36debe1a6068-0', usage_metadata={'input_tokens': 23, 'output_tokens': 78, 'total_tokens': 101}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You are Ankur, and you deliver Gen AI trainings. ðŸ˜Š  I seem to be remembering that information correctly! \\n\\nIs there anything else you'd like to tell me about your work, or perhaps a specific Gen AI topic you're interested in discussing?  I'm here to chat!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 117, 'total_tokens': 182, 'completion_time': 0.118181818, 'prompt_time': 0.005364004, 'queue_time': 0.26285172700000003, 'total_time': 0.123545822}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac369026-6bb2-4280-b313-db3a687cec00-0', usage_metadata={'input_tokens': 117, 'output_tokens': 65, 'total_tokens': 182})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name or what you do. If you'd like to tell me, I'm happy to learn!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 17, 'total_tokens': 58, 'completion_time': 0.074545455, 'prompt_time': 0.001929078, 'queue_time': 0.258541852, 'total_time': 0.076474533}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1e0e7a13-376b-4e2d-ab74-4183ddf5f804-0', usage_metadata={'input_tokens': 17, 'output_tokens': 41, 'total_tokens': 58}), HumanMessage(content='My name is Jhon', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Jhon, it's nice to meet you!  What can I do for you today? ðŸ˜Š \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 71, 'total_tokens': 98, 'completion_time': 0.049090909, 'prompt_time': 0.004126124, 'queue_time': 0.305708846, 'total_time': 0.053217033}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--c1164a18-80f9-4618-8dea-6814184122fc-0', usage_metadata={'input_tokens': 71, 'output_tokens': 27, 'total_tokens': 98}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You told me your name is Jhon.  \\n\\nSince I don't have access to any information about you outside of our current conversation, I don't know what you do. \\n\\nWould you like to tell me? ðŸ˜Š  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 113, 'total_tokens': 165, 'completion_time': 0.094545455, 'prompt_time': 0.005238704, 'queue_time': 0.26003697600000003, 'total_time': 0.099784159}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--0f6ef5a4-628c-43b3-bfe1-0b17199183ef-0', usage_metadata={'input_tokens': 113, 'output_tokens': 52, 'total_tokens': 165})]),\n",
       " 'chat3': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and don't know your name.  \\n\\nWhat can I call you? ðŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 29, 'total_tokens': 62, 'completion_time': 0.06, 'prompt_time': 0.002082678, 'queue_time': 0.263287962, 'total_time': 0.062082678}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--86bff853-8b4e-4be1-8562-13b26baeff2d-0', usage_metadata={'input_tokens': 29, 'output_tokens': 33, 'total_tokens': 62}), HumanMessage(content='My name is Arun', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Arun!\\n\\nIs there anything I can help you with today?  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 73, 'total_tokens': 95, 'completion_time': 0.04, 'prompt_time': 0.013229136, 'queue_time': 2.437003765, 'total_time': 0.053229136}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1df84820-18b7-44c9-b9ee-31214928a0a9-0', usage_metadata={'input_tokens': 73, 'output_tokens': 22, 'total_tokens': 95}), HumanMessage(content='Whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='You told me your name is Arun!  ðŸ˜Š \\n\\nDo you need me to remember anything else?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 107, 'total_tokens': 131, 'completion_time': 0.043636364, 'prompt_time': 0.005394285, 'queue_time': 0.259955795, 'total_time': 0.049030649}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--dbfc8940-cf44-4823-ad0b-821ab40e57c8-0', usage_metadata={'input_tokens': 107, 'output_tokens': 24, 'total_tokens': 131})])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Whats my name?\n",
      "AI: As an AI, I have no memory of past conversations and don't know your name.  \n",
      "\n",
      "What can I call you? ðŸ˜Š  \n",
      "\n",
      "\n",
      "Human: My name is Arun\n",
      "AI: Nice to meet you, Arun!\n",
      "\n",
      "Is there anything I can help you with today?  \n",
      "\n",
      "Human: Whats my name?\n",
      "AI: You told me your name is Arun!  ðŸ˜Š \n",
      "\n",
      "Do you need me to remember anything else?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(store['chat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You introduced yourself as Arun, and we're now chatting. \\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Can you summarize the previous conversations in 15 words\")] , config = config3)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding some more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\" , \"You are a helpful assistant. Answer all the questions to the best of your ability in {language}\"),\n",
    "     MessagesPlaceholder(variable_name=\"messages\")]\n",
    "    )\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Namaste Ajay! \\n\\nà¤®à¥à¤à¥‡ à¤†à¤ªà¤•à¥€ à¤®à¤¦à¤¦ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤–à¥à¤¶à¥€ à¤¹à¥‹ à¤°à¤¹à¥€ à¤¹à¥ˆà¥¤ à¤†à¤ª à¤œà¥‹ à¤­à¥€ à¤¸à¤µà¤¾à¤² à¤ªà¥‚à¤›à¥‡à¤‚à¤—à¥‡, à¤®à¥ˆà¤‚ à¤…à¤ªà¤¨à¥‡ à¤œà¥à¤žà¤¾à¤¨ à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤° à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤¨à¥‡ à¤•à¥€ à¤ªà¥‚à¤°à¥€ à¤•à¥‹à¤¶à¤¿à¤¶ à¤•à¤°à¥‚à¤à¤—à¤¾à¥¤\\n\\nà¤†à¤ªà¤•à¥‡ à¤¸à¤µà¤¾à¤²? \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\" : [HumanMessage(content = \"My name is Ajay\")], \"language\" : \"Hindi\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will wrap the above logic in Message History class becuase there are multiple keys/place holders in input. We need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain,get_session_history,input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Â¡Hola Piyush!  \\n\\nEncantado de conocerte. Â¿CÃ³mo puedo ayudarte? ðŸ˜Š  \\n\\nNo dudes en preguntarme cualquier cosa, harÃ© todo lo posible para responderte en espaÃ±ol.  \\n\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\" : {\"session_id\" : \"chat4\"}}\n",
    "response = with_message_history.invoke({\"messages\" : [HumanMessage(content=\"My name is Piyush\")] , \n",
    "                                        \"language\" : \"Spanish\"},\n",
    "                                        config = config)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat1': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, My name is Ankur and I deliver Gen AI trainings.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Ankur,\\n\\nIt's nice to meet you! \\n\\nThat's fascinating work you do. Delivering Gen AI trainings must be incredibly rewarding. What kind of trainings do you focus on? Are they for technical professionals, business leaders, or a broader audience? \\n\\nI'm always eager to learn more about how people are using and understanding AI.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 23, 'total_tokens': 101, 'completion_time': 0.141818182, 'prompt_time': 0.002150428, 'queue_time': 0.26064566100000003, 'total_time': 0.14396861}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--44d3a052-553d-45ff-896d-36debe1a6068-0', usage_metadata={'input_tokens': 23, 'output_tokens': 78, 'total_tokens': 101}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You are Ankur, and you deliver Gen AI trainings. ðŸ˜Š  I seem to be remembering that information correctly! \\n\\nIs there anything else you'd like to tell me about your work, or perhaps a specific Gen AI topic you're interested in discussing?  I'm here to chat!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 117, 'total_tokens': 182, 'completion_time': 0.118181818, 'prompt_time': 0.005364004, 'queue_time': 0.26285172700000003, 'total_time': 0.123545822}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac369026-6bb2-4280-b313-db3a687cec00-0', usage_metadata={'input_tokens': 117, 'output_tokens': 65, 'total_tokens': 182})]),\n",
       " 'chat2': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and do not know your name or what you do. If you'd like to tell me, I'm happy to learn!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 17, 'total_tokens': 58, 'completion_time': 0.074545455, 'prompt_time': 0.001929078, 'queue_time': 0.258541852, 'total_time': 0.076474533}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1e0e7a13-376b-4e2d-ab74-4183ddf5f804-0', usage_metadata={'input_tokens': 17, 'output_tokens': 41, 'total_tokens': 58}), HumanMessage(content='My name is Jhon', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello Jhon, it's nice to meet you!  What can I do for you today? ðŸ˜Š \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 71, 'total_tokens': 98, 'completion_time': 0.049090909, 'prompt_time': 0.004126124, 'queue_time': 0.305708846, 'total_time': 0.053217033}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--c1164a18-80f9-4618-8dea-6814184122fc-0', usage_metadata={'input_tokens': 71, 'output_tokens': 27, 'total_tokens': 98}), HumanMessage(content='Whats my name and what I do?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You told me your name is Jhon.  \\n\\nSince I don't have access to any information about you outside of our current conversation, I don't know what you do. \\n\\nWould you like to tell me? ðŸ˜Š  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 113, 'total_tokens': 165, 'completion_time': 0.094545455, 'prompt_time': 0.005238704, 'queue_time': 0.26003697600000003, 'total_time': 0.099784159}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--0f6ef5a4-628c-43b3-bfe1-0b17199183ef-0', usage_metadata={'input_tokens': 113, 'output_tokens': 52, 'total_tokens': 165})]),\n",
       " 'chat3': InMemoryChatMessageHistory(messages=[HumanMessage(content='Whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As an AI, I have no memory of past conversations and don't know your name.  \\n\\nWhat can I call you? ðŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 29, 'total_tokens': 62, 'completion_time': 0.06, 'prompt_time': 0.002082678, 'queue_time': 0.263287962, 'total_time': 0.062082678}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--86bff853-8b4e-4be1-8562-13b26baeff2d-0', usage_metadata={'input_tokens': 29, 'output_tokens': 33, 'total_tokens': 62}), HumanMessage(content='My name is Arun', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, Arun!\\n\\nIs there anything I can help you with today?  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 73, 'total_tokens': 95, 'completion_time': 0.04, 'prompt_time': 0.013229136, 'queue_time': 2.437003765, 'total_time': 0.053229136}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--1df84820-18b7-44c9-b9ee-31214928a0a9-0', usage_metadata={'input_tokens': 73, 'output_tokens': 22, 'total_tokens': 95}), HumanMessage(content='Whats my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='You told me your name is Arun!  ðŸ˜Š \\n\\nDo you need me to remember anything else?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 107, 'total_tokens': 131, 'completion_time': 0.043636364, 'prompt_time': 0.005394285, 'queue_time': 0.259955795, 'total_time': 0.049030649}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--dbfc8940-cf44-4823-ad0b-821ab40e57c8-0', usage_metadata={'input_tokens': 107, 'output_tokens': 24, 'total_tokens': 131}), HumanMessage(content='Can you summarize the previous conversations in 15 words', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You introduced yourself as Arun, and we're now chatting. \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 150, 'total_tokens': 167, 'completion_time': 0.030909091, 'prompt_time': 0.007074673, 'queue_time': 0.259624452, 'total_time': 0.037983764}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--40309640-2d8a-4acd-af6c-3e2a2f8650cf-0', usage_metadata={'input_tokens': 150, 'output_tokens': 17, 'total_tokens': 167})])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing the Conversation History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages, SystemMessage\n",
    "\n",
    "trimmer = trim_messages(max_tokens = 10, strategy = \"last\", token_counter = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\Movate_AgenticAI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='I am fine', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content = \"You are a good assistant\"),\n",
    "    HumanMessage(content=\"Hi, I am Bob\"),\n",
    "    AIMessage(content=\"Hi Bob\"),\n",
    "    HumanMessage(content=\"I like vanilla icecream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"what is 2+2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem\"),\n",
    "    HumanMessage(content=\"How are you?\"),\n",
    "    AIMessage(content=\"I am fine\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function wrapped in module langchain_core.messages.utils:\n",
      "\n",
      "wrapped(messages: 'Union[Sequence[MessageLikeRepresentation], None]' = None, **kwargs: 'Any') -> 'Union[list[BaseMessage], Runnable[Sequence[MessageLikeRepresentation], list[BaseMessage]]]'\n",
      "    Trim messages to be below a token count.\n",
      "    \n",
      "    trim_messages can be used to reduce the size of a chat history to a specified token\n",
      "    count or specified message count.\n",
      "    \n",
      "    In either case, if passing the trimmed chat history back into a chat model\n",
      "    directly, the resulting chat history should usually satisfy the following\n",
      "    properties:\n",
      "    \n",
      "    1. The resulting chat history should be valid. Most chat models expect that chat\n",
      "       history starts with either (1) a `HumanMessage` or (2) a `SystemMessage` followed\n",
      "       by a `HumanMessage`. To achieve this, set `start_on=\"human\"`.\n",
      "       In addition, generally a `ToolMessage` can only appear after an `AIMessage`\n",
      "       that involved a tool call.\n",
      "       Please see the following link for more information about messages:\n",
      "       https://python.langchain.com/docs/concepts/#messages\n",
      "    2. It includes recent messages and drops old messages in the chat history.\n",
      "       To achieve this set the `strategy=\"last\"`.\n",
      "    3. Usually, the new chat history should include the `SystemMessage` if it\n",
      "       was present in the original chat history since the `SystemMessage` includes\n",
      "       special instructions to the chat model. The `SystemMessage` is almost always\n",
      "       the first message in the history if present. To achieve this set the\n",
      "       `include_system=True`.\n",
      "    \n",
      "    **Note** The examples below show how to configure `trim_messages` to achieve\n",
      "        a behavior consistent with the above properties.\n",
      "    \n",
      "    Args:\n",
      "        messages: Sequence of Message-like objects to trim.\n",
      "        max_tokens: Max token count of trimmed messages.\n",
      "        token_counter: Function or llm for counting tokens in a BaseMessage or a list of\n",
      "            BaseMessage. If a BaseLanguageModel is passed in then\n",
      "            BaseLanguageModel.get_num_tokens_from_messages() will be used.\n",
      "            Set to `len` to count the number of **messages** in the chat history.\n",
      "    \n",
      "            .. note::\n",
      "                Use `count_tokens_approximately` to get fast, approximate token counts.\n",
      "                This is recommended for using `trim_messages` on the hot path, where\n",
      "                exact token counting is not necessary.\n",
      "    \n",
      "        strategy: Strategy for trimming.\n",
      "            - \"first\": Keep the first <= n_count tokens of the messages.\n",
      "            - \"last\": Keep the last <= n_count tokens of the messages.\n",
      "            Default is \"last\".\n",
      "        allow_partial: Whether to split a message if only part of the message can be\n",
      "            included. If ``strategy=\"last\"`` then the last partial contents of a message\n",
      "            are included. If ``strategy=\"first\"`` then the first partial contents of a\n",
      "            message are included.\n",
      "            Default is False.\n",
      "        end_on: The message type to end on. If specified then every message after the\n",
      "            last occurrence of this type is ignored. If ``strategy==\"last\"`` then this\n",
      "            is done before we attempt to get the last ``max_tokens``. If\n",
      "            ``strategy==\"first\"`` then this is done after we get the first\n",
      "            ``max_tokens``. Can be specified as string names (e.g. \"system\", \"human\",\n",
      "            \"ai\", ...) or as BaseMessage classes (e.g. SystemMessage, HumanMessage,\n",
      "            AIMessage, ...). Can be a single type or a list of types.\n",
      "            Default is None.\n",
      "        start_on: The message type to start on. Should only be specified if\n",
      "            ``strategy=\"last\"``. If specified then every message before\n",
      "            the first occurrence of this type is ignored. This is done after we trim\n",
      "            the initial messages to the last ``max_tokens``. Does not\n",
      "            apply to a SystemMessage at index 0 if ``include_system=True``. Can be\n",
      "            specified as string names (e.g. \"system\", \"human\", \"ai\", ...) or as\n",
      "            BaseMessage classes (e.g. SystemMessage, HumanMessage, AIMessage, ...). Can\n",
      "            be a single type or a list of types.\n",
      "            Default is None.\n",
      "        include_system: Whether to keep the SystemMessage if there is one at index 0.\n",
      "            Should only be specified if ``strategy=\"last\"``.\n",
      "            Default is False.\n",
      "        text_splitter: Function or ``langchain_text_splitters.TextSplitter`` for\n",
      "            splitting the string contents of a message. Only used if\n",
      "            ``allow_partial=True``. If ``strategy=\"last\"`` then the last split tokens\n",
      "            from a partial message will be included. if ``strategy==\"first\"`` then the\n",
      "            first split tokens from a partial message will be included. Token splitter\n",
      "            assumes that separators are kept, so that split contents can be directly\n",
      "            concatenated to recreate the original text. Defaults to splitting on\n",
      "            newlines.\n",
      "    \n",
      "    Returns:\n",
      "        list of trimmed BaseMessages.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: if two incompatible arguments are specified or an unrecognized\n",
      "            ``strategy`` is specified.\n",
      "    \n",
      "    Example:\n",
      "        Trim chat history based on token count, keeping the SystemMessage if\n",
      "        present, and ensuring that the chat history starts with a HumanMessage (\n",
      "        or a SystemMessage followed by a HumanMessage).\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            from langchain_core.messages import (\n",
      "                AIMessage,\n",
      "                HumanMessage,\n",
      "                BaseMessage,\n",
      "                SystemMessage,\n",
      "                trim_messages,\n",
      "            )\n",
      "    \n",
      "            messages = [\n",
      "                SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
      "                HumanMessage(\"i wonder why it's called langchain\"),\n",
      "                AIMessage(\n",
      "                    'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
      "                ),\n",
      "                HumanMessage(\"and who is harrison chasing anyways\"),\n",
      "                AIMessage(\n",
      "                    \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
      "                ),\n",
      "                HumanMessage(\"what do you call a speechless parrot\"),\n",
      "            ]\n",
      "    \n",
      "    \n",
      "            trim_messages(\n",
      "                messages,\n",
      "                max_tokens=45,\n",
      "                strategy=\"last\",\n",
      "                token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
      "                # Most chat models expect that chat history starts with either:\n",
      "                # (1) a HumanMessage or\n",
      "                # (2) a SystemMessage followed by a HumanMessage\n",
      "                start_on=\"human\",\n",
      "                # Usually, we want to keep the SystemMessage\n",
      "                # if it's present in the original history.\n",
      "                # The SystemMessage has special instructions for the model.\n",
      "                include_system=True,\n",
      "                allow_partial=False,\n",
      "            )\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [\n",
      "                SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
      "                HumanMessage(content='what do you call a speechless parrot'),\n",
      "            ]\n",
      "    \n",
      "        Trim chat history based on the message count, keeping the SystemMessage if\n",
      "        present, and ensuring that the chat history starts with a HumanMessage (\n",
      "        or a SystemMessage followed by a HumanMessage).\n",
      "    \n",
      "            trim_messages(\n",
      "                messages,\n",
      "                # When `len` is passed in as the token counter function,\n",
      "                # max_tokens will count the number of messages in the chat history.\n",
      "                max_tokens=4,\n",
      "                strategy=\"last\",\n",
      "                # Passing in `len` as a token counter function will\n",
      "                # count the number of messages in the chat history.\n",
      "                token_counter=len,\n",
      "                # Most chat models expect that chat history starts with either:\n",
      "                # (1) a HumanMessage or\n",
      "                # (2) a SystemMessage followed by a HumanMessage\n",
      "                start_on=\"human\",\n",
      "                # Usually, we want to keep the SystemMessage\n",
      "                # if it's present in the original history.\n",
      "                # The SystemMessage has special instructions for the model.\n",
      "                include_system=True,\n",
      "                allow_partial=False,\n",
      "            )\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            [\n",
      "                SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
      "                HumanMessage(content='and who is harrison chasing anyways'),\n",
      "                AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n",
      "                HumanMessage(content='what do you call a speechless parrot'),\n",
      "            ]\n",
      "    \n",
      "    \n",
      "        Trim chat history using a custom token counter function that counts the\n",
      "        number of tokens in each message.\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            messages = [\n",
      "                SystemMessage(\"This is a 4 token text. The full message is 10 tokens.\"),\n",
      "                HumanMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"first\"),\n",
      "                AIMessage(\n",
      "                    [\n",
      "                        {\"type\": \"text\", \"text\": \"This is the FIRST 4 token block.\"},\n",
      "                        {\"type\": \"text\", \"text\": \"This is the SECOND 4 token block.\"},\n",
      "                    ],\n",
      "                    id=\"second\",\n",
      "                ),\n",
      "                HumanMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"third\"),\n",
      "                AIMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"fourth\"),\n",
      "            ]\n",
      "    \n",
      "            def dummy_token_counter(messages: list[BaseMessage]) -> int:\n",
      "                # treat each message like it adds 3 default tokens at the beginning\n",
      "                # of the message and at the end of the message. 3 + 4 + 3 = 10 tokens\n",
      "                # per message.\n",
      "    \n",
      "                default_content_len = 4\n",
      "                default_msg_prefix_len = 3\n",
      "                default_msg_suffix_len = 3\n",
      "    \n",
      "                count = 0\n",
      "                for msg in messages:\n",
      "                    if isinstance(msg.content, str):\n",
      "                        count += default_msg_prefix_len + default_content_len + default_msg_suffix_len\n",
      "                    if isinstance(msg.content, list):\n",
      "                        count += default_msg_prefix_len + len(msg.content) *  default_content_len + default_msg_suffix_len\n",
      "                return count\n",
      "    \n",
      "        First 30 tokens, allowing partial messages:\n",
      "            .. code-block:: python\n",
      "    \n",
      "                trim_messages(\n",
      "                    messages,\n",
      "                    max_tokens=30,\n",
      "                    token_counter=dummy_token_counter,\n",
      "                    strategy=\"first\",\n",
      "                    allow_partial=True,\n",
      "                )\n",
      "    \n",
      "            .. code-block:: python\n",
      "    \n",
      "                [\n",
      "                    SystemMessage(\"This is a 4 token text. The full message is 10 tokens.\"),\n",
      "                    HumanMessage(\"This is a 4 token text. The full message is 10 tokens.\", id=\"first\"),\n",
      "                    AIMessage( [{\"type\": \"text\", \"text\": \"This is the FIRST 4 token block.\"}], id=\"second\"),\n",
      "                ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trim_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
